{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Chandramadi/fine_tuning_bert-base-multilingual-cased/blob/main/Fine_Tuning_bert_base_multilingual_cased.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 1: Install All Required Dependencies"
      ],
      "metadata": {
        "id": "QshI4E5WdIXV"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X5JTsRlSdD5O"
      },
      "outputs": [],
      "source": [
        "# Core transformers and datasets\n",
        "!pip install -q transformers datasets accelerate scikit-learn\n",
        "\n",
        "# LoRA + quantization support (bitsandbytes + PEFT)\n",
        "!pip install bitsandbytes\n",
        "!pip install -q peft"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import bitsandbytes as bnb\n",
        "from google.colab import userdata\n",
        "import os\n",
        "from datasets import Dataset"
      ],
      "metadata": {
        "id": "hYhMv_r2hJ_6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 2: Load mBERT Model & Tokenizer (Quantized + Lora)"
      ],
      "metadata": {
        "id": "UUDO4bOTdVS3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from transformers import AutoModelForSequenceClassification, AutoTokenizer, BitsAndBytesConfig\n",
        "from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n",
        "\n",
        "model_name = \"bert-base-multilingual-cased\"  # this is the correct path, not \"google-bert/...\"\n",
        "\n",
        "# Define quantization config\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_8bit=True,\n",
        "    llm_int8_enable_fp32_cpu_offload=True,  # enables CPU fallback\n",
        "    bnb_4bit_use_double_quant=False,\n",
        "    bnb_4bit_quant_type=\"nf4\",\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        ")\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "\n",
        "label2id = {\"bn\": 0, \"gu\": 1, \"hi\": 2, \"english\": 3}\n",
        "id2label = {0: \"bn\", 1: \"gu\", 2: \"hi\", 3: \"english\"}\n",
        "\n",
        "model = AutoModelForSequenceClassification.from_pretrained(\n",
        "    model_name,\n",
        "    quantization_config=bnb_config,\n",
        "    device_map=\"auto\",\n",
        "    num_labels=4,  # Important!\n",
        "    id2label=id2label,\n",
        "    label2id=label2id,\n",
        ")\n",
        "\n",
        "# Prepare for LoRA fine-tuning\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "\n",
        "# Add LoRA adapters\n",
        "lora_config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=16,\n",
        "    target_modules=[\"query\", \"value\"],\n",
        "    lora_dropout=0.1,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_CLS\"\n",
        ")\n",
        "\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "id": "ht4m4WXMdaII"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 3 — load and preprocess the samanantar dataset"
      ],
      "metadata": {
        "id": "oQQC9FahmYq1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extract and Prepare Samples from the Dataset"
      ],
      "metadata": {
        "id": "R_hbiwPjo6VP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# Load multiple language datasets from Samanantar\n",
        "dataset = {}\n",
        "languages = ['bn', 'gu', 'hi']\n",
        "for lang in languages:\n",
        "    dataset[lang] = load_dataset(\"ai4bharat/samanantar\", lang, split=\"train[:1000]\")"
      ],
      "metadata": {
        "id": "adytgabEmavS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Now combine the text and labels:"
      ],
      "metadata": {
        "id": "3MpURCJNpC5E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create text-label pairs\n",
        "\n",
        "langs = ['bn', 'gu', 'hi']\n",
        "label2id = {lang: i for i, lang in enumerate(langs)}\n",
        "label2id['english'] = len(label2id)  # Add english as a new class\n",
        "\n",
        "samples = []\n",
        "\n",
        "# Original Indian language samples (tgt as text)\n",
        "for lang in langs:\n",
        "    lang_data = dataset[lang]\n",
        "    for row in lang_data:\n",
        "        samples.append({\n",
        "            \"text\": row[\"tgt\"],  # Indian language sentence\n",
        "            \"label\": label2id[lang]\n",
        "        })\n",
        "\n",
        "# English samples (src as text)\n",
        "for lang in langs:\n",
        "    lang_data = dataset[lang]\n",
        "    for row in lang_data:\n",
        "        samples.append({\n",
        "            \"text\": row[\"src\"],  # English sentence\n",
        "            \"label\": label2id[\"english\"]\n",
        "        })\n",
        "raw_dataset = Dataset.from_list(samples)\n",
        "print(raw_dataset)\n"
      ],
      "metadata": {
        "id": "FoLUukringoX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Split Your Dataset & Tokenize Correctly into train, valid, and test."
      ],
      "metadata": {
        "id": "eY05XY1DpW-t"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "# 1. Split raw dataset\n",
        "train_testvalid = raw_dataset.train_test_split(test_size=0.1, seed=42)\n",
        "test_valid = train_testvalid['test'].train_test_split(test_size=0.5, seed=42)\n",
        "\n",
        "final_dataset = DatasetDict({\n",
        "    'train': train_testvalid['train'],\n",
        "    'validation': test_valid['train'],\n",
        "    'test': test_valid['test'],\n",
        "})\n",
        "\n",
        "# 2. Tokenization function\n",
        "def tokenize_function(example):\n",
        "    return tokenizer(example[\"text\"], padding=\"max_length\", truncation=True)\n",
        "\n",
        "# 3. Tokenize datasets\n",
        "tokenized_datasets = final_dataset.map(tokenize_function, batched=True)\n",
        "\n",
        "# 4. Rename 'label' to 'labels'\n",
        "tokenized_datasets = tokenized_datasets.rename_column(\"label\", \"labels\")\n",
        "\n",
        "# 5. Set format for PyTorch\n",
        "tokenized_datasets.set_format(type=\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "# 6. Extract subsets for training/eval/test\n",
        "train_dataset = tokenized_datasets[\"train\"]\n",
        "eval_dataset = tokenized_datasets[\"validation\"]\n",
        "test_dataset = tokenized_datasets[\"test\"]\n",
        "\n",
        "# 7. Optional sanity check\n",
        "print(train_dataset[0])\n"
      ],
      "metadata": {
        "id": "VBCFw9-NpZo5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 4 - Model Training using Hugging Face Transformers"
      ],
      "metadata": {
        "id": "kM5SgcP2srI_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.1 - Import the Trainer and related utilities:"
      ],
      "metadata": {
        "id": "JMY_3xX44CyH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import Trainer, TrainingArguments, EvalPrediction\n",
        "import numpy as np\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "metadata": {
        "id": "4gjLErbLss59"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.2 - Define a metrics function:"
      ],
      "metadata": {
        "id": "fORCItDm4JtM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_metrics(eval_pred: EvalPrediction):\n",
        "    logits, labels = eval_pred\n",
        "    predictions = np.argmax(logits, axis=1)\n",
        "    acc = accuracy_score(labels, predictions)\n",
        "    return {\"accuracy\": acc}\n"
      ],
      "metadata": {
        "id": "Hc1DG37B4HB-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###  4.3 - Setup training arguments:"
      ],
      "metadata": {
        "id": "qcNUjcBu4PM9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./results\",\n",
        "    per_device_train_batch_size=8,\n",
        "    per_device_eval_batch_size=8,\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    logging_dir=\"./logs\",\n",
        "    num_train_epochs=3,\n",
        "    save_strategy=\"epoch\",\n",
        "    logging_steps=10,\n",
        "    learning_rate=2e-4,\n",
        "    remove_unused_columns=False,  # Required for LoRA + PEFT\n",
        "    fp16=True,\n",
        "    save_total_limit=2\n",
        ")\n"
      ],
      "metadata": {
        "id": "8NqBll_E4QSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.4 - Create the Trainer:"
      ],
      "metadata": {
        "id": "cQIILROI4TQJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=train_dataset,\n",
        "    eval_dataset=eval_dataset,\n",
        "    tokenizer=tokenizer,\n",
        "    compute_metrics=compute_metrics,\n",
        ")\n"
      ],
      "metadata": {
        "id": "j-ln6xUn4V_H"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 4.5 - Start training:"
      ],
      "metadata": {
        "id": "3D_wMWIs4Zke"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\""
      ],
      "metadata": {
        "id": "AFisSAE45hFg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.train()"
      ],
      "metadata": {
        "id": "CdF7P-FY4b3F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 5 — Evaluate on Test Set"
      ],
      "metadata": {
        "id": "LqY6h3bK498N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.evaluate(test_dataset)"
      ],
      "metadata": {
        "id": "jtiFrWhd4_CO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Step 6 - print predicted labels with their confidence scores"
      ],
      "metadata": {
        "id": "egWLh7keGluD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# ✅ Mapping of IDs to actual language labels (4 labels including English)\n",
        "id2label = {0: 'bn', 1: 'gu', 2: 'hi', 3: 'en'}\n",
        "\n",
        "# 🔁 Input sentence(s)\n",
        "sentences = [\n",
        "    \"Prime Minister Modi met the French President today.\",  # English\n",
        "    \"इनपुट उपकरण को ऑनलाइन आज़माएं\",  # Hindi\n",
        "    \"প্রধানমন্ত্রী আজ ফরাসি রাষ্ট্রপতির সঙ্গে সাক্ষাৎ করেছেন\",  # Bengali\n",
        "    \"પ્રધાનમંત્રીએ આજે ફ્રાંસના રાષ્ટ્રપતિ સાથે બેઠક કરી\",  # Gujarati\n",
        "    \"helllo\",  # Junk/unknown\n",
        "]\n",
        "\n",
        "# ✅ Tokenize the sentences\n",
        "inputs = tokenizer(sentences, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "# ✅ Forward pass through model\n",
        "with torch.no_grad():\n",
        "    outputs = model(**inputs)\n",
        "\n",
        "# ✅ Apply softmax to get probabilities\n",
        "probs = F.softmax(outputs.logits, dim=-1)\n",
        "\n",
        "# ✅ Get predicted class IDs\n",
        "preds = torch.argmax(probs, dim=-1)\n",
        "\n",
        "# 🔍 Confidence threshold (adjustable)\n",
        "CONFIDENCE_THRESHOLD = 0.80\n",
        "\n",
        "# 🖨️ Print sentence, predicted label, confidence, and all class probabilities\n",
        "for i, (sent, prob, pred_id) in enumerate(zip(sentences, probs, preds)):\n",
        "    confidence = prob[pred_id].item()\n",
        "    label = id2label[pred_id.item()] if confidence >= CONFIDENCE_THRESHOLD else \"unknown\"\n",
        "    class_probs = {id2label[j]: round(p.item(), 4) for j, p in enumerate(prob)}\n",
        "\n",
        "    print(f\"Sentence {i+1}: {sent}\")\n",
        "    print(f\"➡️ Predicted: {label} | Confidence: {confidence:.4f}\")\n",
        "    print(f\"📊 Class Probabilities: {class_probs}\\n\")\n"
      ],
      "metadata": {
        "id": "BA21S1256zgX"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}